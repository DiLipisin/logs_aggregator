# logs_aggregator

### Дано 
N больших файлов с неупорядоченными данными со строками заданного формата.

### Задача
Сагрегировать эти данные и записать в результирующий файл в нужном формате (JSON),
по возрастанию даты действий.

## Алгоритм
В алгоритме 3 этапа
1. Запускается в M потоков, каждый поток обрабатывает один логфайл единовременно.
Логфайлы балансируются между потоками. Для каждого логфайла есть мапа <file_name, ostream>. 
file_name - унифицированное название файла, составленное из даты действия. Внути файла 
хранятся неуникальные стороки со названием действия и его 
свойствами через запятую [fact_name|prop0, ..., prop9]. Файлы хранятся в отдельной 
директории для каждого потока. В каждом потоке выполняется следующее:  
    a) считывается одна строчка файла;  
    b) десериализуется в JSON;   
    c) из даты формируется название файла, а fact_name и props формируется строка для записи;   
    d) проверяется существование названия файла в мапе, если его нет, то он создается 
    и открывается, затем в конец файла записывается тапл;  
2. После того как все начальные файлы обработаны, необходимо смерждить файлы разных потоков, 
соответствующие одной дате. Для этого в N потоков делаем следующее:  
    a) агрегируем названия одноименных файлов (с одинаковыми датами) из разных директорий 
    (для разных потоков), созданных 1.  
    b) последовательно считываем пачки одноименных файлов в структуру, подсчитывая props, 
    записываем в новые файлы (для каждой даты отдельно) готовые строки формата
    "some_fact_name": [{"props": [...], "count": 1}, ...] (фактически это слияние файлов)  
3. Проходим временным файлам из 2 этапа, отсортированным по названию (дате) и записываем в 
результирующий файл строчки из полуподготовленных файлов из 2b).  
    
## Преимущества решения
1. Логфайлы обрабатываются параллельно в несколько потоков, что улучшает быстроту выполнения
по сравнению с последовательной обработкой  
2. В памяти единовременно не хранятся все распаршенные записи логфайлов 

## Недостатки решения
1. В памяти единовременно хранится файл с агрегированными записями для одной даты, в 
вырожденном случае это могут быть записи со всех логфайлов  
2. Большая часть времени уходит на построчное чтение записей логфайлов, их парсинг и запись 
во временные файлы, ассоциированные с датами действий. Наиболее вероятно, это связано с 
библиотекой для работы с JSON  

## Сборка и запуск приложения
```
$ mkdir build
$ cd build
$ cmake -DCMAKE_BUILD_TYPE=Release ..
$ cmake --build . -- -j4
$ ./bin/logs /tmp/log_dir 10 10 /tmp/res_dir
```
Arguments:
1 - log files directory
2 - files number
3 - threads number
4 - result output directory

## Запуск тестов
```
...logs/tests$ pytest-3 -vvs ./
```

#### Результаты тестирования
Обрабатывалось 10 файлов по 100.000 рандомно сгенерированных записей.

| Потоки | 1 этап, c | 2 этап, c | Общее время, с | 
| ------ | --------- | --------- | -------------- | 
| 1      | ~38       | ~1        | ~39            |  
| 4      | ~41       | ~1        | ~42            |   
| 10     | ~93       | ~4        | ~97            |  

Здесь видно, что первый этап занимает больше всего времени, пути улучшения приведены в 
отдельном разделе.

## Пути улучшения
1. Сравнить потокобезопасные библиотеки для работы с JSON и, заменить jsoncpp (постфактум 
поняла, что выбрала [неудачную либу](https://github.com/miloyip/nativejson-benchmark/blob/master/sample/performance_Corei7-4980HQ@2.80GHz_mac64_clang7.0_1._Parse_Time_(ms).png)).  
2. Использовать библиотеку для логирования.  
3. Если в результирующий файл допустимо записывать даты не в порядке возрастания, то можно не 
формировать агрегированные промежуточные файлы, а сразу записывать данные в результирующий 
файл.  

### Начальные предположения и алгоритм
##### Предположения
1. Исходя из предметной области, скорее всего действий ограниченное количество 
(худший порядок - сотни), дней в году также ограниченное количество (также сотни), а логи чаще 
всего парсят не реже, чем раз в год.  
2. Наименьший размер файла в linux 4 Кб, и, предположительно, это и есть средний размер файла 
с props для конкретной даты и действия.  
3. Из 1 и 2 пунктов вытекает, что даже если действий 1000, дней 300, а размер файла 4 Кб,
то размер занимаемой памяти будет 1,2 Гб.

##### Алгоритм
Алгоритм похож на реализованный за исключением того, что записи логфайлов изначально делятся 
по дате и названию действия (пример названия файла extra_super_fact_name_2020-09-30), это 
позволило бы при слиянии файлов по дате и действию считывать в память меньше данных. Однако 
здесь мне не хватило файловых дескрипторов, которых в linux'е 1024 на процесс.  

### Другой алгоритм
Принимает в качестве аргумента число половины всех потоков, если введено M, то всего 2*М потоков.
Глобально приложение состоит из: 
1. M потоков, читающих логфайлы последовательно и записывающих в очереди, причем все записи 
с конкретными датой и действием кладутся в одну и ту же очередь (считается хеш);
2. M очередей, в которые кладутся распаршенные записи файлов;
3. M потоков, читающих каждый из своей очереди и записывающих props в отдельные файлы, 
ассоциированные с конкретными датой и действием (названия файлов формируются как date_factname).

Таким образом,
1. M файлов читают записи из файлов, парсят и пушат в очереди, с другой стороны очереди M потоков 
распределяют эти записи по временным файлам по дате и действию.
2. После завершения чтения из всех логфайлов временные файлы сортируются по названию.
3. Затем итерируясь по отсортированным файлам, считываем все записи props из каждого временного 
файла, подсчитываем уникальные последовательности props и записываем в результирующий файл 
в формате JSON
