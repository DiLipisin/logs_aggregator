#Logs
#### Example of my coding style

### Дано 
N больших файлов с неупорядоченными данными со строками заданного формата

### Задача
сагригировать эти данные и записать в результирующий файл в нужном форма

### Предположения
1. Исходя из предметной области, скорее всего действий ограниченное количество 
(худший порядок - сотни), дней в году также ограниченное количество (также сотни).  
2. Нименьший размер файла в linux 4 Кб, и предположительно также это и средний размер.  
3. Из 1 и 2 пунктов вытекает, что даже если действий 1000, дней 300, а размер файла 4 Кб,
то размер занимаемой памяти будет 1,2 Гб

## Алгоритм
1. Каждый файл обрабатывается в отдельном потоке, у потока есть мапа <file_name, ostream>. 
file_name - унифицированное название файла, составленное из даты и названия действия. Внути 
файла хранятся неуникальные стороки со свойствами (prop0, ..., prop9) через запятую, файлы 
хранятся в отдельной директории для каждого потока. В каждом 
потоке выполняется следующее:  
    a) считывается одна строчка файла  
    b) десериализуется в JSON  
    c) из даты и названия действия формируется название файла, а из тапла prop формируется 
    строка через запятую  
    d) проверяется существование сформированного файла в мапе, если его нет, то он создается 
    и открывается, затем в конец файла записывается тапл
2. После того как все начальные файлы обработаны, необходимо смерждить файлы разных потоков, 
соответствующие одной дате и одному действию. Для этого в N потоков делаем следующее:  
    a) берем файл из первой директории (первого потока), считываем строки-таплы в мапу 
    <prop, counter>  
    b) ищем других директориях файлы с аналогичными названиями, читаем, дополняем мапу  
    c) записываем мапу в результирующий файл в нужном формате  
    