# logs_aggregator

### Дано 
N больших файлов с неупорядоченными данными со строками заданного формата

### Задача
Сагрегировать эти данные и записать в результирующий файл в нужном формате (JSON),
по возрвстанию даты действий

## Алгоритм
1. Поток обрабатывает один файл единовременно, для каждого логфайла есть мапа 
<file_name, ostream>. file_name - унифицированное название файла, составленное из даты 
действия. Внути файла хранятся неуникальные стороки со названием действия и его 
свойствами через запятую [fact_name|prop0, ..., prop9]. Файлы хранятся в отдельной 
директории для каждого потока. В каждом потоке выполняется следующее:  
    a) считывается одна строчка файла  
    b) десериализуется в JSON  
    c) из даты формируется название файла, а fact_name и props формируется строка для записи    
    d) проверяется существование названия файла в мапе, если его нет, то он создается 
    и открывается, затем в конец файла записывается тапл
2. После того как все начальные файлы обработаны, необходимо смерждить файлы разных потоков, 
соответствующие одной дате. Для этого в N потоков делаем следующее:  
    a) агрегируем названия одноименных файлов (с одинаковыми датами) из разных директорий 
    (для разных потоков), созданных 1.
    b) последовательно считываем пачки одноименных файлов в структуру, подсчитывая props, 
    записываем в новые файлы (для каждой даты отдельно) готовые строки формата
    "some_fact_name": [{"props": [...], "count": 1}, ...] (фактически это слияние файлов)  
    c) проходим по файлам, отсортированным по названию (дате) и записываем в результирующий
    файл строчки из полуподготовленных файлов из b).  
    
## Преимущества решения
1. Логфайлы обрабатываются парллельно в несколько потоков, что улучшает быстроту выполнения
поравнению с последовательной обработкой  
2. В памяти единовременно не хрянятся все распаршенные записи логфайлов  

## Недостатки решения
1. В памяти единовременно хрянится файл с агрегированными записями для одной даты, в 
вырожденном случае это могут быть записи со всех логфайлов  
2. Большая часть времени уходит на построчное чтение записей логфайлов, их парсинг и запись 
во временные файлы, ассоциированные с датами действий. Возможно, это связвно с библиотекой для 
работы с JSON  

### Начальные предположения и алгоритм
##### Предположения
1. Исходя из предметной области, скорее всего действий ограниченное количество 
(худший порядок - сотни), дней в году также ограниченное количество (также сотни).  
2. Нименьший размер файла в linux 4 Кб, и предположительно также это и средний размер.  
3. Из 1 и 2 пунктов вытекает, что даже если действий 1000, дней 300, а размер файла 4 Кб,
то размер занимаемой памяти будет 1,2 Гб

##### Алгоритм
Алгоритм похож на реализванный за исключением того, что записи логфайлов изначально делятся 
по дате и названию действия (пример названия файла extra_super_fact_name_2020-09-30), это 
позволило бы при слиянии файлов по дате и действию считывать в память меньше данных. Однако 
здесь мне не хватило файловых дескрипторов, которых в linux'е 1024 на процесс.  

## Сборка и запуск приложения
```
$ mkdir build
$ cd build
$ cmake -DCMAKE_BUILD_TYPE=Release ..
$ cmake --build . -- -j4
$ ./bin/logs /tmp/log_dir 10 10 /tmp/res
```
Arguments:
1 - log files directory
2 - files number
3 - threads number
4 - result output directory

## Запуск тестов
```
...logs/tests$ pytest-3 -vvs ./
```

#### Результаты тестирования
Обрабатывалось 10 файлов по 100.000 рандомно сгенерированных записей

| Потоки | 1 этап, c | 2 этап, c | Общее время, с | 
| ------ | --------- | --------- | -------------- | 
| 1      | ~38       | ~1        | ~39            |  
| 4      | ~41       | ~1        | ~42            |   
| 10     | ~93       | ~4        | ~97            |  

## Пути улучшения
1. Сравнить потокобезопасные библиотеки для работы с JSON и, возможно, заменить jsoncpp  
2. Использовать библиотеку для логирования  
3. Если в результирующий файл допустимо записывать даты не в порядке возрастания, то можно не 
формировать агрегированные промежуточные файлы, а сразу записывать данные в результирующий 
файл.  
